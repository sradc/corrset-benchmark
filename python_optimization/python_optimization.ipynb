{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- line_profiler is used for relative speeds of lines, but slows down the overall execution, that's why iteration speed is measured separately\n",
    "\n",
    "\n",
    "- TODO: improve var names, e.g. \"numba\" for numba, etc.\n",
    "- TODO: add markdown notes and include them in the report. i.e. generate a markdown report that can paste into blog. (need to be able to run in command line because faster than jupyter)\n",
    "\n",
    "\n",
    "Idea:\n",
    "- maybe faster to pass in questions and users seperately, rather than scores matrix, which is large... idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "import math\n",
    "import time\n",
    "from math import sqrt\n",
    "from typing import Iterable\n",
    "\n",
    "import line_profiler\n",
    "import numba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import IndexSlice as islice\n",
    "\n",
    "\n",
    "K = 5\n",
    "DEFAULT_MAX_ITER = 1000\n",
    "BEST_OF = 3\n",
    "IS_ITER_SCALED = False  # scale up the number of iterations if fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"data/large.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_func(outer_func: callable, func_to_profile: callable) -> str:\n",
    "    lp = line_profiler.LineProfiler()\n",
    "    outer_func_with_profiler = lp(outer_func)\n",
    "    lp.add_function(func_to_profile)\n",
    "    outer_func_with_profiler()\n",
    "    stream = io.StringIO()\n",
    "    lp.print_stats(stream=stream)\n",
    "    report = stream.getvalue()\n",
    "    # Format report string\n",
    "    s = \"\"\n",
    "    # Get first table of report\n",
    "    add = False\n",
    "    for line in report.split(\"\\n\"):\n",
    "        if f\"Function: {func_to_profile.__name__}\" in line:\n",
    "            add = True\n",
    "        if add:\n",
    "            s += line + \"\\n\"\n",
    "        if add and \"Total time\" in line:\n",
    "            break\n",
    "    # Get only the % Time column onwards\n",
    "    start_idx = s.find(\"% Time\")\n",
    "    for line in s.split(\"\\n\"):\n",
    "        if \"Line #\" in line:\n",
    "            start_idx = line.index(\"% Time\")\n",
    "            break\n",
    "    s = \"\\n\".join(x[start_idx:] for x in s.split(\"\\n\"))\n",
    "    return s\n",
    "\n",
    "\n",
    "def fmt_seconds(seconds: float) -> str | None:\n",
    "    units = [\n",
    "        (1e-9, \"ns\"),\n",
    "        (1e-6, \"Î¼s\"),\n",
    "        (1e-3, \"ms\"),\n",
    "        (1, \"sec\"),\n",
    "    ]\n",
    "    for threshold, unit in reversed(units):\n",
    "        if seconds > threshold:\n",
    "            time_in_unit = seconds / threshold\n",
    "            if time_in_unit > 100:\n",
    "                time_in_unit = 10 * round(time_in_unit / 10)\n",
    "            elif time_in_unit > 10:\n",
    "                time_in_unit = round(time_in_unit)\n",
    "            else:\n",
    "                time_in_unit = round(time_in_unit * 10) / 10\n",
    "            return f\"{time_in_unit} {unit}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original code - baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_corrset(data, max_iter, K):\n",
    "    qs_iter = itertools.islice(\n",
    "        itertools.combinations(data.question.unique(), K), max_iter\n",
    "    )\n",
    "    q_to_score = data.set_index([\"question\", \"user\"])\n",
    "    grand_totals = data.groupby(\"user\").score.sum().rename(\"grand_total\")\n",
    "    start = time.time()\n",
    "    corrs = compute_corrs(qs_iter, q_to_score, grand_totals)\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame(corrs)\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "def compute_corrs(\n",
    "    qs_iter: Iterable, q_to_score: pd.DataFrame, grand_totals: pd.DataFrame\n",
    "):\n",
    "    result = []\n",
    "    for qs in qs_iter:\n",
    "        qs_data = q_to_score.loc[islice[qs, :], :].swaplevel()\n",
    "        answered_all = qs_data.groupby(level=[0]).size() == K\n",
    "        answered_all = answered_all[answered_all].index\n",
    "        qs_total = (\n",
    "            qs_data.loc[islice[answered_all, :]]\n",
    "            .groupby(level=[0])\n",
    "            .sum()\n",
    "            .rename(columns={\"score\": \"qs\"})\n",
    "        )\n",
    "        r = qs_total.join(grand_totals).corr().qs.grand_total\n",
    "        result.append({\"qs\": qs, \"r\": r})\n",
    "    return result\n",
    "\n",
    "\n",
    "print(f\"Computing corrs_baseline, avg_iter_time_secs_baseline...\")\n",
    "avg_iter_time_secs_baseline = math.inf\n",
    "for _ in range(BEST_OF):\n",
    "    corrs_baseline, avg_iter_time_secs = k_corrset(data, max_iter=DEFAULT_MAX_ITER, K=K)\n",
    "    avg_iter_time_secs_baseline = min(avg_iter_time_secs_baseline, avg_iter_time_secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_count = 0\n",
    "\n",
    "\n",
    "def run_benchmark(\n",
    "    k_corrset_func,\n",
    "    compute_corrs_func,\n",
    "    description,\n",
    "    num_iter=None,\n",
    "    run_line_profiler=True,\n",
    "):\n",
    "    global benchmark_count\n",
    "    benchmark_count += 1\n",
    "    print(f\"Benchmark #{benchmark_count}: {description}\")\n",
    "\n",
    "    # Test values for correctness, and possibly warmup jit\n",
    "    corrs, _ = k_corrset_func(data, K=K, max_iter=DEFAULT_MAX_ITER)\n",
    "    assert np.allclose(corrs_baseline.r, corrs.r, equal_nan=True)\n",
    "\n",
    "    if num_iter is None and IS_ITER_SCALED:\n",
    "        # scale number of iters based on speedup, for more reliable timing\n",
    "        _, avg_iter_time_secs = k_corrset_func(data, K=K, max_iter=DEFAULT_MAX_ITER)\n",
    "        multiplier = avg_iter_time_secs_baseline / avg_iter_time_secs\n",
    "        num_iter = int(DEFAULT_MAX_ITER * multiplier)\n",
    "    num_iter = num_iter or DEFAULT_MAX_ITER\n",
    "    print(f\"Using {num_iter} iterations...\")\n",
    "\n",
    "    # Measure iteration time\n",
    "    best_avg_iter_time_secs = math.inf\n",
    "    for _ in range(BEST_OF):\n",
    "        _, avg_iter_time_secs = k_corrset_func(data, K=K, max_iter=num_iter)\n",
    "        best_avg_iter_time_secs = min(best_avg_iter_time_secs, avg_iter_time_secs)\n",
    "    print(f\"\\nAvg time per iteration:  {fmt_seconds(best_avg_iter_time_secs)}\")\n",
    "    print(\n",
    "        f\"Speedup over baseline:   {avg_iter_time_secs_baseline/best_avg_iter_time_secs:0.1f}x\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    if run_line_profiler:\n",
    "        s = profile_func(\n",
    "            lambda: k_corrset_func(data, K=K, max_iter=DEFAULT_MAX_ITER),\n",
    "            compute_corrs_func,\n",
    "        )\n",
    "        print(s.strip())\n",
    "        print()\n",
    "\n",
    "\n",
    "run_benchmark(k_corrset, compute_corrs, \"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_corrset(data, K, max_iter=1000):\n",
    "    qs_iter = itertools.islice(\n",
    "        itertools.combinations(data.question.unique(), K), max_iter\n",
    "    )\n",
    "    users_who_answered_q = {q: set() for q in data.question.unique()}\n",
    "    for q, u in data[[\"question\", \"user\"]].itertuples(index=False):\n",
    "        users_who_answered_q[q].add(u)\n",
    "    q_to_score = data.set_index([\"question\", \"user\"])\n",
    "    grand_totals = data.groupby(\"user\").score.sum().rename(\"grand_total\")\n",
    "    start = time.time()\n",
    "    corrs = compute_corrs(qs_iter, users_who_answered_q, q_to_score, grand_totals)\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame(corrs)\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "def compute_corrs(qs_iter, users_who_answered_q, q_to_score, grand_totals):\n",
    "    result = []\n",
    "    for qs in qs_iter:\n",
    "        user_sets_for_qs = [users_who_answered_q[q] for q in qs]\n",
    "        answered_all = set.intersection(*user_sets_for_qs)\n",
    "        qs_data = q_to_score.loc[islice[qs, :], :].swaplevel()\n",
    "        qs_total = (\n",
    "            qs_data.loc[islice[list(answered_all), :]]\n",
    "            .groupby(level=[0])\n",
    "            .sum()\n",
    "            .rename(columns={\"score\": \"qs\"})\n",
    "        )\n",
    "        r = qs_total.join(grand_totals).corr().qs.grand_total\n",
    "        result.append({\"qs\": qs, \"r\": r})\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset,\n",
    "    compute_corrs,\n",
    "    \"Dict of sets to identify users who answered qs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_corrset(data, K, max_iter=1000):\n",
    "    qs_iter = itertools.islice(\n",
    "        itertools.combinations(data.question.unique(), K), max_iter\n",
    "    )\n",
    "    users_who_answered_q = {q: set() for q in data.question.unique()}\n",
    "    for q, u in data[[\"question\", \"user\"]].itertuples(index=False):\n",
    "        users_who_answered_q[q].add(u)\n",
    "    score_dict = {\n",
    "        (q, u): s\n",
    "        for q, u, s in data[[\"question\", \"user\", \"score\"]].itertuples(index=False)\n",
    "    }\n",
    "    grand_totals = data.groupby(\"user\").score.sum().rename(\"grand_total\")\n",
    "    start = time.time()\n",
    "    corrs = compute_corrs(qs_iter, users_who_answered_q, score_dict, grand_totals)\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame(corrs)\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "def compute_corrs(qs_iter, users_who_answered_q, score_dict, grand_totals):\n",
    "    result = []\n",
    "    for qs in qs_iter:\n",
    "        user_sets_for_qs = [users_who_answered_q[q] for q in qs]\n",
    "        answered_all = set.intersection(*user_sets_for_qs)\n",
    "        qs_total = {u: sum(score_dict[q, u] for q in qs) for u in answered_all}\n",
    "        qs_total = pd.DataFrame.from_dict(qs_total, orient=\"index\", columns=[\"qs\"])\n",
    "        qs_total.index.name = \"user\"\n",
    "        r = qs_total.join(grand_totals).corr().qs.grand_total\n",
    "        result.append({\"qs\": qs, \"r\": r})\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset,\n",
    "    compute_corrs,\n",
    "    \"Dict to look up scores\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_corrset(data, K, max_iter=1000):\n",
    "    qs_iter = itertools.islice(\n",
    "        itertools.combinations(data.question.unique(), K), max_iter\n",
    "    )\n",
    "    users_who_answered_q = {q: set() for q in data.question.unique()}\n",
    "    for q, u in data[[\"question\", \"user\"]].itertuples(index=False):\n",
    "        users_who_answered_q[q].add(u)\n",
    "    score_dict = {\n",
    "        (q, u): s\n",
    "        for q, u, s in data[[\"question\", \"user\", \"score\"]].itertuples(index=False)\n",
    "    }\n",
    "    grand_totals = data.groupby(\"user\").score.sum().rename(\"grand_total\").to_dict()\n",
    "    start = time.time()\n",
    "    corrs = compute_corrs(qs_iter, users_who_answered_q, score_dict, grand_totals)\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame(corrs)\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "def compute_corrs(qs_iter, users_who_answered_q, score_dict, grand_totals):\n",
    "    result = []\n",
    "    for qs in qs_iter:\n",
    "        user_sets_for_qs = [users_who_answered_q[q] for q in qs]\n",
    "        answered_all = set.intersection(*user_sets_for_qs)\n",
    "        qs_total = [sum(score_dict[q, u] for q in qs) for u in answered_all]\n",
    "        user_grand_total = [grand_totals[u] for u in answered_all]\n",
    "        r = np.corrcoef(qs_total, user_grand_total)[0, 1]\n",
    "        result.append({\"qs\": qs, \"r\": r})\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(k_corrset, compute_corrs, \"Dict to look up grand totals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data.user = data.user.map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data.question = data.question.map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "    qs_iter = itertools.islice(\n",
    "        itertools.combinations(data.question.unique(), K), max_iter\n",
    "    )\n",
    "    users_who_answered_q = {q: set() for q in data.question.unique()}\n",
    "    for q, u in data[[\"question\", \"user\"]].itertuples(index=False):\n",
    "        users_who_answered_q[q].add(u)\n",
    "    score_dict = {\n",
    "        (q, u): s\n",
    "        for q, u, s in data[[\"question\", \"user\", \"score\"]].itertuples(index=False)\n",
    "    }\n",
    "    grand_totals = data.groupby(\"user\").score.sum().rename(\"grand_total\").to_dict()\n",
    "    start = time.time()\n",
    "    corrs = compute_corrs(qs_iter, users_who_answered_q, score_dict, grand_totals)\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame(corrs)\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "def compute_corrs(qs_iter, users_who_answered_q, score_dict, grand_totals):\n",
    "    result = []\n",
    "    for qs in qs_iter:\n",
    "        user_sets_for_qs = [users_who_answered_q[q] for q in qs]\n",
    "        answered_all = set.intersection(*user_sets_for_qs)\n",
    "        qs_total = [sum(score_dict[q, u] for q in qs) for u in answered_all]\n",
    "        user_grand_total = [grand_totals[u] for u in answered_all]\n",
    "        r = np.corrcoef(qs_total, user_grand_total)[0, 1]\n",
    "        result.append({\"qs\": qs, \"r\": r})\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(k_corrset, compute_corrs, \"Replacing long string UUIDs with ints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data.user = data.user.map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data.question = data.question.map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "    qs_iter = itertools.islice(\n",
    "        itertools.combinations(data.question.unique(), K), max_iter\n",
    "    )\n",
    "    users_who_answered_q = np.zeros(\n",
    "        (len(data.question.unique()), len(data.user.unique())), dtype=np.bool_\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].itertuples(index=False):\n",
    "        users_who_answered_q[q, u] = True\n",
    "    score_dict = {\n",
    "        (q, u): s\n",
    "        for q, u, s in data[[\"question\", \"user\", \"score\"]].itertuples(index=False)\n",
    "    }\n",
    "    grand_totals = data.groupby(\"user\").score.sum().rename(\"grand_total\").to_dict()\n",
    "    start = time.time()\n",
    "    corrs = compute_corrs(qs_iter, users_who_answered_q, score_dict, grand_totals)\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame(corrs)\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "def compute_corrs(qs_iter, users_who_answered_q, score_dict, grand_totals):\n",
    "    result = []\n",
    "    for qs in qs_iter:\n",
    "        user_sets_for_qs = users_who_answered_q[qs, :]  # numpy indexing\n",
    "        answered_all = np.logical_and.reduce(user_sets_for_qs)\n",
    "        answered_all = np.where(answered_all)[0]\n",
    "        qs_total = [sum(score_dict[q, u] for q in qs) for u in answered_all]\n",
    "        user_grand_total = [grand_totals[u] for u in answered_all]\n",
    "        r = np.corrcoef(qs_total, user_grand_total)[0, 1]\n",
    "        result.append({\"qs\": qs, \"r\": r})\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset, compute_corrs, \"NumPy bool_ array to identify users who answered qs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data.user = data.user.map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data.question = data.question.map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "    qs_iter = itertools.islice(\n",
    "        itertools.combinations(data.question.unique(), K), max_iter\n",
    "    )\n",
    "    users_who_answered_q = np.zeros(\n",
    "        (len(data.question.unique()), len(data.user.unique())), dtype=np.bool_\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].itertuples(index=False):\n",
    "        users_who_answered_q[q, u] = True\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (len(data.user.unique()), len(data.question.unique())), dtype=np.float64\n",
    "    )\n",
    "    for q, u, s in data[[\"question\", \"user\", \"score\"]].itertuples(index=False):\n",
    "        score_matrix[u, q] = s\n",
    "\n",
    "    grand_totals = data.groupby(\"user\").score.sum().rename(\"grand_total\").to_dict()\n",
    "    start = time.time()\n",
    "    corrs = compute_corrs(qs_iter, users_who_answered_q, score_matrix, grand_totals)\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame(corrs)\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "def compute_corrs(qs_iter, users_who_answered_q, score_matrix, grand_totals):\n",
    "    result = []\n",
    "    for qs in qs_iter:\n",
    "        user_sets_for_qs = users_who_answered_q[qs, :]\n",
    "        answered_all = np.logical_and.reduce(user_sets_for_qs)\n",
    "        answered_all = np.where(answered_all)[0]\n",
    "        qs_total = score_matrix[answered_all, :][:, qs].sum(axis=1)\n",
    "        user_grand_total = [grand_totals[u] for u in answered_all]\n",
    "        r = np.corrcoef(qs_total, user_grand_total)[0, 1]\n",
    "        result.append({\"qs\": qs, \"r\": r})\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset, compute_corrs, \"Score matrix instead of dict to look up scores\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data.user = data.user.map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data.question = data.question.map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "    qs_iter = itertools.islice(\n",
    "        itertools.combinations(data.question.unique(), K), max_iter\n",
    "    )\n",
    "    users_who_answered_q = np.zeros(\n",
    "        (len(data.question.unique()), len(data.user.unique())), dtype=np.bool_\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].itertuples(index=False):\n",
    "        users_who_answered_q[q, u] = True\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (len(data.user.unique()), len(data.question.unique())), dtype=np.float64\n",
    "    )\n",
    "    for q, u, s in data[[\"question\", \"user\", \"score\"]].itertuples(index=False):\n",
    "        score_matrix[u, q] = s\n",
    "\n",
    "    grand_totals = data.groupby(\"user\").score.sum().rename(\"grand_total\").to_dict()\n",
    "    start = time.time()\n",
    "    corrs = compute_corrs(qs_iter, users_who_answered_q, score_matrix, grand_totals)\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame(corrs)\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "def corrcoef(a: list[float], b: list[float]) -> float | None:\n",
    "    \"\"\"same as np.corrcoef(a, b)[0, 1]\"\"\"\n",
    "    n = len(a)\n",
    "    sum_a = sum(a)\n",
    "    sum_b = sum(b)\n",
    "    sum_ab = sum(a_i * b_i for a_i, b_i in zip(a, b))\n",
    "    sum_a_sq = sum(a_i**2 for a_i in a)\n",
    "    sum_b_sq = sum(b_i**2 for b_i in b)\n",
    "    num = n * sum_ab - sum_a * sum_b\n",
    "    den = sqrt(n * sum_a_sq - sum_a**2) * sqrt(n * sum_b_sq - sum_b**2)\n",
    "    if den == 0:\n",
    "        return None\n",
    "    return num / den\n",
    "\n",
    "\n",
    "def compute_corrs(qs_iter, users_who_answered_q, score_matrix, grand_totals):\n",
    "    result = []\n",
    "    for qs in qs_iter:\n",
    "        user_sets_for_qs = users_who_answered_q[qs, :]  # numpy indexing\n",
    "        answered_all = np.logical_and.reduce(user_sets_for_qs)\n",
    "        answered_all = np.where(answered_all)[0]\n",
    "        qs_total = score_matrix[answered_all, :][:, qs].sum(axis=1)\n",
    "        user_grand_total = [grand_totals[u] for u in answered_all]\n",
    "        r = corrcoef(qs_total, user_grand_total)\n",
    "        result.append({\"qs\": qs, \"r\": r})\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(k_corrset, compute_corrs, \"Custom corrcoef function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data.user = data.user.map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data.question = data.question.map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "    qs_iter = itertools.islice(\n",
    "        itertools.combinations(data.question.unique(), K), max_iter\n",
    "    )\n",
    "    qs_iter = np.array(list(qs_iter))\n",
    "    users_who_answered_q = np.zeros(\n",
    "        (len(data.question.unique()), len(data.user.unique())), dtype=np.bool_\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].itertuples(index=False):\n",
    "        users_who_answered_q[q, u] = True\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (len(data.user.unique()), len(data.question.unique())), dtype=np.float64\n",
    "    )\n",
    "    for q, u, s in data[[\"question\", \"user\", \"score\"]].itertuples(index=False):\n",
    "        score_matrix[u, q] = s\n",
    "\n",
    "    grand_totals = data.groupby(\"user\").score.sum().rename(\"grand_total\").to_dict()\n",
    "    start = time.time()\n",
    "    corrs = compute_corrs(qs_iter, users_who_answered_q, score_matrix, grand_totals)\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame(corrs)\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "def compute_corrs(qs_combinations, users_who_answered_q, score_matrix, grand_totals):\n",
    "    result = []\n",
    "    for i in range(len(qs_combinations)):\n",
    "        qs = qs_combinations[i]\n",
    "        user_sets_for_qs = users_who_answered_q[qs, :]  # numpy indexing\n",
    "        answered_all = np.logical_and.reduce(user_sets_for_qs)\n",
    "        answered_all = np.where(answered_all)[0]\n",
    "        qs_total = score_matrix[answered_all, :][:, qs].sum(axis=1)\n",
    "        user_grand_total = [grand_totals[u] for u in answered_all]\n",
    "        r = corrcoef(qs_total, user_grand_total)\n",
    "        result.append({\"qs\": qs, \"r\": r})\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset, compute_corrs, \"Pass qs_combinations as numpy array (prep for numba)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data.user = data.user.map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data.question = data.question.map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "    qs_combinations = itertools.islice(\n",
    "        itertools.combinations(data.question.unique(), K), max_iter\n",
    "    )\n",
    "    qs_combinations = np.array(list(qs_combinations))\n",
    "    users_who_answered_q = np.zeros(\n",
    "        (len(data.question.unique()), len(data.user.unique())), dtype=np.bool_\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].itertuples(index=False):\n",
    "        users_who_answered_q[q, u] = True\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (len(data.user.unique()), len(data.question.unique())), dtype=np.float64\n",
    "    )\n",
    "    for q, u, s in data[[\"question\", \"user\", \"score\"]].itertuples(index=False):\n",
    "        score_matrix[u, q] = s\n",
    "\n",
    "    grand_totals = data.groupby(\"user\").score.sum().rename(\"grand_total\").to_dict()\n",
    "    start = time.time()\n",
    "    r_vals = compute_corrs(\n",
    "        qs_combinations, users_who_answered_q, score_matrix, grand_totals\n",
    "    )\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame({\"qs\": [tuple(qs) for qs in qs_combinations], \"r\": r_vals})\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "def corrcoef(a: list[float], b: list[float]) -> float | None:\n",
    "    \"\"\"same as np.corrcoef(a, b)[0, 1]\"\"\"\n",
    "    n = len(a)\n",
    "    sum_a = sum(a)\n",
    "    sum_b = sum(b)\n",
    "    sum_ab = sum(a_i * b_i for a_i, b_i in zip(a, b))\n",
    "    sum_a_sq = sum(a_i**2 for a_i in a)\n",
    "    sum_b_sq = sum(b_i**2 for b_i in b)\n",
    "    num = n * sum_ab - sum_a * sum_b\n",
    "    den = sqrt(n * sum_a_sq - sum_a**2) * sqrt(n * sum_b_sq - sum_b**2)\n",
    "    if den == 0:\n",
    "        return None\n",
    "    return num / den\n",
    "\n",
    "\n",
    "def compute_corrs(qs_iter, users_who_answered_q, score_matrix, grand_totals):\n",
    "    result = np.empty(len(qs_iter), dtype=np.float64)\n",
    "    for i in range(len(qs_iter)):\n",
    "        qs = qs_iter[i]\n",
    "        user_sets_for_qs = users_who_answered_q[qs, :]  # numpy indexing\n",
    "        answered_all = np.logical_and.reduce(user_sets_for_qs)\n",
    "        answered_all = np.where(answered_all)[0]\n",
    "        qs_total = score_matrix[answered_all, :][:, qs].sum(axis=1)\n",
    "        user_grand_total = [grand_totals[u] for u in answered_all]\n",
    "        result[i] = corrcoef(qs_total, user_grand_total)\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(k_corrset, compute_corrs, \"Result array instead of list (prep for numba)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data.user = data.user.map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data.question = data.question.map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "    qs_combinations = itertools.islice(\n",
    "        itertools.combinations(data.question.unique(), K), max_iter\n",
    "    )\n",
    "    qs_combinations = np.array(list(qs_combinations))\n",
    "    users_who_answered_q = np.zeros(\n",
    "        (len(data.question.unique()), len(data.user.unique())), dtype=np.bool_\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].itertuples(index=False):\n",
    "        users_who_answered_q[q, u] = True\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (len(data.user.unique()), len(data.question.unique())), dtype=np.float64\n",
    "    )\n",
    "    for q, u, s in data[[\"question\", \"user\", \"score\"]].itertuples(index=False):\n",
    "        score_matrix[u, q] = s\n",
    "\n",
    "    grand_totals = score_matrix.sum(axis=1)\n",
    "\n",
    "    start = time.time()\n",
    "    r_vals = compute_corrs(\n",
    "        qs_combinations, users_who_answered_q, score_matrix, grand_totals\n",
    "    )\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame({\"qs\": [tuple(qs) for qs in qs_combinations], \"r\": r_vals})\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def corrcoef_numba(a, b):\n",
    "    \"\"\"same as np.corrcoef(a, b)[0, 1]\"\"\"\n",
    "    n = len(a)\n",
    "    sum_a = sum(a)\n",
    "    sum_b = sum(b)\n",
    "    sum_ab = sum(a * b)\n",
    "    sum_a_sq = sum(a * a)\n",
    "    sum_b_sq = sum(b * b)\n",
    "    num = n * sum_ab - sum_a * sum_b\n",
    "    den = math.sqrt(n * sum_a_sq - sum_a**2) * math.sqrt(n * sum_b_sq - sum_b**2)\n",
    "    return np.nan if den == 0 else num / den\n",
    "\n",
    "\n",
    "@numba.njit(parallel=False)\n",
    "def compute_corrs(qs_combinations, users_who_answered_q, score_matrix, grand_totals):\n",
    "    result = np.empty(len(qs_combinations), dtype=np.float64)\n",
    "    for i in range(len(qs_combinations)):\n",
    "        qs = qs_combinations[i]\n",
    "        user_sets_for_qs = users_who_answered_q[qs, :]\n",
    "        # numba doesn't support np.logical_and.reduce\n",
    "        answered_all = user_sets_for_qs[0]\n",
    "        for j in range(1, len(user_sets_for_qs)):\n",
    "            answered_all *= user_sets_for_qs[j]\n",
    "        answered_all = np.where(answered_all)[0]\n",
    "        qs_total = score_matrix[answered_all, :][:, qs].sum(axis=1)\n",
    "        user_grand_total = grand_totals[answered_all]\n",
    "        result[i] = corrcoef_numba(qs_total, user_grand_total)\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset,\n",
    "    compute_corrs,\n",
    "    \"naive numba, with parallel=False, (no support for np.logical_and.reduce)\",\n",
    "    run_line_profiler=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit(parallel=True)\n",
    "def compute_corrs(qs_combinations, users_who_answered_q, score_matrix, grand_totals):\n",
    "    result = np.empty(len(qs_combinations), dtype=np.float64)\n",
    "    for i in numba.prange(len(qs_combinations)):\n",
    "        qs = qs_combinations[i]\n",
    "        user_sets_for_qs = users_who_answered_q[qs, :]\n",
    "        # numba doesn't support np.logical_and.reduce\n",
    "        answered_all = user_sets_for_qs[0]\n",
    "        for j in range(1, len(user_sets_for_qs)):\n",
    "            answered_all *= user_sets_for_qs[j]\n",
    "        answered_all = np.where(answered_all)[0]\n",
    "        qs_total = score_matrix[answered_all, :][:, qs].sum(axis=1)\n",
    "        user_grand_total = grand_totals[answered_all]\n",
    "        result[i] = corrcoef_numba(qs_total, user_grand_total)\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset,\n",
    "    compute_corrs,\n",
    "    \"naive numba, with parallel=True, (no support for np.logical_and.reduce)\",\n",
    "    run_line_profiler=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring on the bitsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Let's go back and try bitsets, with our basic python code.\n",
    "# or maybe not.. it's slow in normal python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitset_to_list(arr):\n",
    "    result = []\n",
    "    for idx in range(arr.shape[0]):\n",
    "        if arr[idx] == 0:\n",
    "            continue\n",
    "        for pos in range(64):\n",
    "            if (arr[idx] & (np.int64(1) << np.int64(pos))) != 0:\n",
    "                result.append(idx * 64 + pos)\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def bitset_create(size):\n",
    "    size_in_int64 = int(np.ceil(size / 64))\n",
    "    return np.zeros(size_in_int64, dtype=np.int64)\n",
    "\n",
    "\n",
    "def bitset_add(arr, pos):\n",
    "    int64_idx = pos // 64\n",
    "    pos_in_int64 = pos % 64\n",
    "    arr[int64_idx] |= np.int64(1) << np.int64(pos_in_int64)\n",
    "\n",
    "\n",
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data[\"user\"] = data[\"user\"].map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data[\"question\"] = data[\"question\"].map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "\n",
    "    all_qs = data.question.unique()\n",
    "    grand_totals = data.groupby(\"user\").score.sum().values\n",
    "\n",
    "    # create bitsets\n",
    "    users_who_answered_q = np.array(\n",
    "        [bitset_create(data.user.nunique()) for _ in range(data.question.nunique())]\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].values:\n",
    "        bitset_add(users_who_answered_q[q], u)\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (data.user.nunique(), data.question.nunique()), dtype=np.int64\n",
    "    )\n",
    "    for row in data.itertuples():\n",
    "        score_matrix[row.user, row.question] = row.score\n",
    "\n",
    "    # todo, would be nice to have a super fast iterator / generator in numba\n",
    "    # rather than creating the whole array\n",
    "    qs_combinations = []\n",
    "    for i, qs in enumerate(itertools.combinations(all_qs, K)):\n",
    "        if i == max_iter:\n",
    "            break\n",
    "        qs_combinations.append(qs)\n",
    "    qs_combinations = np.array(qs_combinations)\n",
    "\n",
    "    start = time.time()\n",
    "    r_vals = compute_corrs(\n",
    "        qs_combinations, users_who_answered_q, score_matrix, grand_totals\n",
    "    )\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame({\"qs\": [tuple(qs) for qs in qs_combinations], \"r\": r_vals})\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "def compute_corrs(qs_combinations, users_who_answered_q, score_matrix, grand_totals):\n",
    "    num_qs = qs_combinations.shape[0]\n",
    "    bitset_size = users_who_answered_q[0].shape[0]\n",
    "    result = np.empty(qs_combinations.shape[0], dtype=np.float64)\n",
    "    for i in range(num_qs):\n",
    "        qs = qs_combinations[i]\n",
    "        user_sets_for_qs = users_who_answered_q[qs_combinations[i]]\n",
    "        answered_all = np.bitwise_and.reduce(user_sets_for_qs)\n",
    "        answered_all = bitset_to_list(answered_all)\n",
    "        qs_total = score_matrix[answered_all, :][:, qs].sum(axis=1)\n",
    "        user_grand_total = grand_totals[answered_all]\n",
    "        result[i] = corrcoef(qs_total, user_grand_total)\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset,\n",
    "    compute_corrs,\n",
    "    \"bitsets, with no numba at all\",\n",
    "    run_line_profiler=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def bitset_to_list(arr):\n",
    "    result = []\n",
    "    for idx in range(arr.shape[0]):\n",
    "        if arr[idx] == 0:\n",
    "            continue\n",
    "        for pos in range(64):\n",
    "            if (arr[idx] & (np.int64(1) << np.int64(pos))) != 0:\n",
    "                result.append(idx * 64 + pos)\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data[\"user\"] = data[\"user\"].map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data[\"question\"] = data[\"question\"].map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "\n",
    "    all_qs = data.question.unique()\n",
    "    grand_totals = data.groupby(\"user\").score.sum().values\n",
    "\n",
    "    # create bitsets\n",
    "    users_who_answered_q = np.array(\n",
    "        [bitset_create(data.user.nunique()) for _ in range(data.question.nunique())]\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].values:\n",
    "        bitset_add(users_who_answered_q[q], u)\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (data.user.nunique(), data.question.nunique()), dtype=np.int64\n",
    "    )\n",
    "    for row in data.itertuples():\n",
    "        score_matrix[row.user, row.question] = row.score\n",
    "\n",
    "    # todo, would be nice to have a super fast iterator / generator in numba\n",
    "    # rather than creating the whole array\n",
    "    qs_combinations = []\n",
    "    for i, qs in enumerate(itertools.combinations(all_qs, K)):\n",
    "        if i == max_iter:\n",
    "            break\n",
    "        qs_combinations.append(qs)\n",
    "    qs_combinations = np.array(qs_combinations)\n",
    "\n",
    "    start = time.time()\n",
    "    r_vals = compute_corrs(\n",
    "        qs_combinations, users_who_answered_q, score_matrix, grand_totals\n",
    "    )\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame({\"qs\": [tuple(qs) for qs in qs_combinations], \"r\": r_vals})\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "def compute_corrs(qs_combinations, users_who_answered_q, score_matrix, grand_totals):\n",
    "    num_qs = qs_combinations.shape[0]\n",
    "    bitset_size = users_who_answered_q[0].shape[0]\n",
    "    result = np.empty(qs_combinations.shape[0], dtype=np.float64)\n",
    "    for i in range(num_qs):\n",
    "        qs = qs_combinations[i]\n",
    "        user_sets_for_qs = users_who_answered_q[qs_combinations[i]]\n",
    "        answered_all = np.bitwise_and.reduce(user_sets_for_qs)\n",
    "        answered_all = bitset_to_list(answered_all)\n",
    "        qs_total = score_matrix[answered_all, :][:, qs].sum(axis=1)\n",
    "        user_grand_total = grand_totals[answered_all]\n",
    "        result[i] = corrcoef(qs_total, user_grand_total)\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset,\n",
    "    compute_corrs,\n",
    "    \"bitsets, with numba on bitset_to_list\",\n",
    "    run_line_profiler=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def bitset_to_list(arr):\n",
    "    result = []\n",
    "    for idx in range(arr.shape[0]):\n",
    "        if arr[idx] == 0:\n",
    "            continue\n",
    "        for pos in range(64):\n",
    "            if (arr[idx] & (np.int64(1) << np.int64(pos))) != 0:\n",
    "                result.append(idx * 64 + pos)\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "# @numba.njit\n",
    "def bitset_create(size):\n",
    "    size_in_int64 = int(np.ceil(size / 64))\n",
    "    return np.zeros(size_in_int64, dtype=np.int64)\n",
    "\n",
    "\n",
    "# @numba.njit\n",
    "def bitset_add(arr, pos):\n",
    "    int64_idx = pos // 64\n",
    "    pos_in_int64 = pos % 64\n",
    "    arr[int64_idx] |= np.int64(1) << np.int64(pos_in_int64)\n",
    "\n",
    "\n",
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data[\"user\"] = data[\"user\"].map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data[\"question\"] = data[\"question\"].map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "\n",
    "    all_qs = data.question.unique()\n",
    "    grand_totals = data.groupby(\"user\").score.sum().values\n",
    "\n",
    "    # create bitsets\n",
    "    users_who_answered_q = np.array(\n",
    "        [bitset_create(data.user.nunique()) for _ in range(data.question.nunique())]\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].values:\n",
    "        bitset_add(users_who_answered_q[q], u)\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (data.user.nunique(), data.question.nunique()), dtype=np.int64\n",
    "    )\n",
    "    for row in data.itertuples():\n",
    "        score_matrix[row.user, row.question] = row.score\n",
    "\n",
    "    # todo, would be nice to have a super fast iterator / generator in numba\n",
    "    # rather than creating the whole array\n",
    "    qs_combinations = []\n",
    "    for i, qs in enumerate(itertools.combinations(all_qs, K)):\n",
    "        if i == max_iter:\n",
    "            break\n",
    "        qs_combinations.append(qs)\n",
    "    qs_combinations = np.array(qs_combinations)\n",
    "\n",
    "    start = time.time()\n",
    "    r_vals = compute_corrs(\n",
    "        qs_combinations, users_who_answered_q, score_matrix, grand_totals\n",
    "    )\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame({\"qs\": [tuple(qs) for qs in qs_combinations], \"r\": r_vals})\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "def compute_corrs(qs_combinations, users_who_answered_q, score_matrix, grand_totals):\n",
    "    num_qs = qs_combinations.shape[0]\n",
    "    bitset_size = users_who_answered_q[0].shape[0]\n",
    "    result = np.empty(qs_combinations.shape[0], dtype=np.float64)\n",
    "    for i in range(num_qs):\n",
    "        qs = qs_combinations[i]\n",
    "        user_sets_for_qs = users_who_answered_q[qs_combinations[i]]\n",
    "        answered_all = np.bitwise_and.reduce(user_sets_for_qs)\n",
    "        answered_all = bitset_to_list(answered_all)\n",
    "        qs_total = score_matrix[answered_all, :][:, qs].sum(axis=1)\n",
    "        user_grand_total = grand_totals[answered_all]\n",
    "        result[i] = corrcoef_numba(qs_total, user_grand_total)\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset,\n",
    "    compute_corrs,\n",
    "    \"numba also on corrcoef\",\n",
    "    run_line_profiler=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def bitset_to_list(arr):\n",
    "    result = []\n",
    "    for idx in range(arr.shape[0]):\n",
    "        if arr[idx] == 0:\n",
    "            continue\n",
    "        for pos in range(64):\n",
    "            if (arr[idx] & (np.int64(1) << np.int64(pos))) != 0:\n",
    "                result.append(idx * 64 + pos)\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "# @numba.njit\n",
    "def bitset_create(size):\n",
    "    size_in_int64 = int(np.ceil(size / 64))\n",
    "    return np.zeros(size_in_int64, dtype=np.int64)\n",
    "\n",
    "\n",
    "# @numba.njit\n",
    "def bitset_add(arr, pos):\n",
    "    int64_idx = pos // 64\n",
    "    pos_in_int64 = pos % 64\n",
    "    arr[int64_idx] |= np.int64(1) << np.int64(pos_in_int64)\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def bitset_and(arrays):\n",
    "    result = arrays[0].copy()\n",
    "    for i in range(1, len(arrays)):\n",
    "        result &= arrays[i]\n",
    "    return result\n",
    "\n",
    "\n",
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data[\"user\"] = data[\"user\"].map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data[\"question\"] = data[\"question\"].map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "\n",
    "    all_qs = data.question.unique()\n",
    "    grand_totals = data.groupby(\"user\").score.sum().values\n",
    "\n",
    "    # create bitsets\n",
    "    users_who_answered_q = np.array(\n",
    "        [bitset_create(data.user.nunique()) for _ in range(data.question.nunique())]\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].values:\n",
    "        bitset_add(users_who_answered_q[q], u)\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (data.user.nunique(), data.question.nunique()), dtype=np.int64\n",
    "    )\n",
    "    for row in data.itertuples():\n",
    "        score_matrix[row.user, row.question] = row.score\n",
    "\n",
    "    # todo, would be nice to have a super fast iterator / generator in numba\n",
    "    # rather than creating the whole array\n",
    "    qs_combinations = []\n",
    "    for i, qs in enumerate(itertools.combinations(all_qs, K)):\n",
    "        if i == max_iter:\n",
    "            break\n",
    "        qs_combinations.append(qs)\n",
    "    qs_combinations = np.array(qs_combinations)\n",
    "\n",
    "    start = time.time()\n",
    "    r_vals = compute_corrs(\n",
    "        qs_combinations, users_who_answered_q, score_matrix, grand_totals\n",
    "    )\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame({\"qs\": [tuple(qs) for qs in qs_combinations], \"r\": r_vals})\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "def compute_corrs(qs_combinations, users_who_answered_q, score_matrix, grand_totals):\n",
    "    num_qs = qs_combinations.shape[0]\n",
    "    bitset_size = users_who_answered_q[0].shape[0]\n",
    "    result = np.empty(qs_combinations.shape[0], dtype=np.float64)\n",
    "    for i in range(num_qs):\n",
    "        qs = qs_combinations[i]\n",
    "        user_sets_for_qs = users_who_answered_q[qs_combinations[i]]\n",
    "        answered_all = bitset_and(user_sets_for_qs)\n",
    "        answered_all = bitset_to_list(answered_all)\n",
    "        qs_total = score_matrix[answered_all, :][:, qs].sum(axis=1)\n",
    "        user_grand_total = grand_totals[answered_all]\n",
    "        result[i] = corrcoef_numba(qs_total, user_grand_total)\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset,\n",
    "    compute_corrs,\n",
    "    \"numba also on bitset_and\",\n",
    "    run_line_profiler=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def bitset_to_list(arr):\n",
    "    result = []\n",
    "    for idx in range(arr.shape[0]):\n",
    "        if arr[idx] == 0:\n",
    "            continue\n",
    "        for pos in range(64):\n",
    "            if (arr[idx] & (np.int64(1) << np.int64(pos))) != 0:\n",
    "                result.append(idx * 64 + pos)\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "# @numba.njit\n",
    "def bitset_create(size):\n",
    "    size_in_int64 = int(np.ceil(size / 64))\n",
    "    return np.zeros(size_in_int64, dtype=np.int64)\n",
    "\n",
    "\n",
    "# @numba.njit\n",
    "def bitset_add(arr, pos):\n",
    "    int64_idx = pos // 64\n",
    "    pos_in_int64 = pos % 64\n",
    "    arr[int64_idx] |= np.int64(1) << np.int64(pos_in_int64)\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def bitset_and(arrays):\n",
    "    result = arrays[0].copy()\n",
    "    for i in range(1, len(arrays)):\n",
    "        result &= arrays[i]\n",
    "    return result\n",
    "\n",
    "\n",
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data[\"user\"] = data[\"user\"].map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data[\"question\"] = data[\"question\"].map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "\n",
    "    all_qs = data.question.unique()\n",
    "    grand_totals = data.groupby(\"user\").score.sum().values\n",
    "\n",
    "    # create bitsets\n",
    "    users_who_answered_q = np.array(\n",
    "        [bitset_create(data.user.nunique()) for _ in range(data.question.nunique())]\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].values:\n",
    "        bitset_add(users_who_answered_q[q], u)\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (data.user.nunique(), data.question.nunique()), dtype=np.int64\n",
    "    )\n",
    "    for row in data.itertuples():\n",
    "        score_matrix[row.user, row.question] = row.score\n",
    "\n",
    "    # todo, would be nice to have a super fast iterator / generator in numba\n",
    "    # rather than creating the whole array\n",
    "    qs_combinations = []\n",
    "    for i, qs in enumerate(itertools.combinations(all_qs, K)):\n",
    "        if i == max_iter:\n",
    "            break\n",
    "        qs_combinations.append(qs)\n",
    "    qs_combinations = np.array(qs_combinations)\n",
    "\n",
    "    start = time.time()\n",
    "    r_vals = compute_corrs(\n",
    "        qs_combinations, users_who_answered_q, score_matrix, grand_totals\n",
    "    )\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame({\"qs\": [tuple(qs) for qs in qs_combinations], \"r\": r_vals})\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "@numba.njit(parallel=False)\n",
    "def compute_corrs(qs_combinations, users_who_answered_q, score_matrix, grand_totals):\n",
    "    num_qs = qs_combinations.shape[0]\n",
    "    bitset_size = users_who_answered_q[0].shape[0]\n",
    "    result = np.empty(qs_combinations.shape[0], dtype=np.float64)\n",
    "    for i in range(num_qs):\n",
    "        qs = qs_combinations[i]\n",
    "        user_sets_for_qs = users_who_answered_q[qs_combinations[i]]\n",
    "        answered_all = bitset_and(user_sets_for_qs)\n",
    "        answered_all = bitset_to_list(answered_all)\n",
    "        qs_total = score_matrix[answered_all, :][:, qs].sum(axis=1)\n",
    "        user_grand_total = grand_totals[answered_all]\n",
    "        result[i] = corrcoef_numba(qs_total, user_grand_total)\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset,\n",
    "    compute_corrs,\n",
    "    \"at this point numpy indexing slow, so numba on the func. parallel=False\",\n",
    "    run_line_profiler=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def bitset_to_list(arr):\n",
    "    result = []\n",
    "    for idx in range(arr.shape[0]):\n",
    "        if arr[idx] == 0:\n",
    "            continue\n",
    "        for pos in range(64):\n",
    "            if (arr[idx] & (np.int64(1) << np.int64(pos))) != 0:\n",
    "                result.append(idx * 64 + pos)\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "# @numba.njit\n",
    "def bitset_create(size):\n",
    "    size_in_int64 = int(np.ceil(size / 64))\n",
    "    return np.zeros(size_in_int64, dtype=np.int64)\n",
    "\n",
    "\n",
    "# @numba.njit\n",
    "def bitset_add(arr, pos):\n",
    "    int64_idx = pos // 64\n",
    "    pos_in_int64 = pos % 64\n",
    "    arr[int64_idx] |= np.int64(1) << np.int64(pos_in_int64)\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def bitset_and(arrays):\n",
    "    result = arrays[0].copy()\n",
    "    for i in range(1, len(arrays)):\n",
    "        result &= arrays[i]\n",
    "    return result\n",
    "\n",
    "\n",
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data[\"user\"] = data[\"user\"].map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data[\"question\"] = data[\"question\"].map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "\n",
    "    all_qs = data.question.unique()\n",
    "    grand_totals = data.groupby(\"user\").score.sum().values\n",
    "\n",
    "    # create bitsets\n",
    "    users_who_answered_q = np.array(\n",
    "        [bitset_create(data.user.nunique()) for _ in range(data.question.nunique())]\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].values:\n",
    "        bitset_add(users_who_answered_q[q], u)\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (data.user.nunique(), data.question.nunique()), dtype=np.int64\n",
    "    )\n",
    "    for row in data.itertuples():\n",
    "        score_matrix[row.user, row.question] = row.score\n",
    "\n",
    "    # todo, would be nice to have a super fast iterator / generator in numba\n",
    "    # rather than creating the whole array\n",
    "    qs_combinations = []\n",
    "    for i, qs in enumerate(itertools.combinations(all_qs, K)):\n",
    "        if i == max_iter:\n",
    "            break\n",
    "        qs_combinations.append(qs)\n",
    "    qs_combinations = np.array(qs_combinations)\n",
    "\n",
    "    start = time.time()\n",
    "    r_vals = compute_corrs(\n",
    "        qs_combinations, users_who_answered_q, score_matrix, grand_totals\n",
    "    )\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame({\"qs\": [tuple(qs) for qs in qs_combinations], \"r\": r_vals})\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "@numba.njit(parallel=True)\n",
    "def compute_corrs(qs_combinations, users_who_answered_q, score_matrix, grand_totals):\n",
    "    num_qs = qs_combinations.shape[0]\n",
    "    bitset_size = users_who_answered_q[0].shape[0]\n",
    "    result = np.empty(qs_combinations.shape[0], dtype=np.float64)\n",
    "    for i in numba.prange(num_qs):\n",
    "        qs = qs_combinations[i]\n",
    "        user_sets_for_qs = users_who_answered_q[qs_combinations[i]]\n",
    "        answered_all = bitset_and(user_sets_for_qs)\n",
    "        answered_all = bitset_to_list(answered_all)\n",
    "        qs_total = score_matrix[answered_all, :][:, qs].sum(axis=1)\n",
    "        user_grand_total = grand_totals[answered_all]\n",
    "        result[i] = corrcoef_numba(qs_total, user_grand_total)\n",
    "    return result\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset,\n",
    "    compute_corrs,\n",
    "    \"at this point numpy indexing slow, so numba on the func. parallel=True\",\n",
    "    run_line_profiler=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def bitset_create(size):\n",
    "    size_in_int64 = int(np.ceil(size / 64))\n",
    "    return np.zeros(size_in_int64, dtype=np.int64)\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def bitset_add(arr, pos):\n",
    "    int64_idx = pos // 64\n",
    "    pos_in_int64 = pos % 64\n",
    "    arr[int64_idx] |= np.int64(1) << np.int64(pos_in_int64)\n",
    "\n",
    "\n",
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data[\"user\"] = data[\"user\"].map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data[\"question\"] = data[\"question\"].map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "\n",
    "    all_qs = data.question.unique()\n",
    "    grand_totals = data.groupby(\"user\").score.sum().values\n",
    "\n",
    "    # create bitsets\n",
    "    users_who_answered_q = np.array(\n",
    "        [bitset_create(data.user.nunique()) for _ in range(data.question.nunique())]\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].values:\n",
    "        bitset_add(users_who_answered_q[q], u)\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (data.user.nunique(), data.question.nunique()), dtype=np.int64\n",
    "    )\n",
    "    for row in data.itertuples():\n",
    "        score_matrix[row.user, row.question] = row.score\n",
    "\n",
    "    # todo, would be nice to have a super fast iterator / generator in numba\n",
    "    # rather than creating the whole array\n",
    "    qs_combinations = []\n",
    "    for i, qs in enumerate(itertools.combinations(all_qs, K)):\n",
    "        if i == max_iter:\n",
    "            break\n",
    "        qs_combinations.append(qs)\n",
    "    qs_combinations = np.array(qs_combinations)\n",
    "\n",
    "    start = time.time()\n",
    "    r_vals = compute_corrs(\n",
    "        qs_combinations, users_who_answered_q, score_matrix, grand_totals\n",
    "    )\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame({\"qs\": [tuple(qs) for qs in qs_combinations], \"r\": r_vals})\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "@numba.njit(boundscheck=False, fastmath=True, parallel=False, nogil=True)\n",
    "def compute_corrs(qs_combinations, users_who_answered_q, score_matrix, grand_totals):\n",
    "    num_qs = qs_combinations.shape[0]\n",
    "    bitset_size = users_who_answered_q[0].shape[0]\n",
    "    corrs = np.empty(qs_combinations.shape[0], dtype=np.float64)\n",
    "    for i in numba.prange(num_qs):\n",
    "        # bitset will contain users who answered all questions in qs_array[i]\n",
    "        bitset = users_who_answered_q[qs_combinations[i, 0]].copy()\n",
    "        for q in qs_combinations[i, 1:]:\n",
    "            bitset &= users_who_answered_q[q]\n",
    "        # retrieve stats for the users to compute correlation\n",
    "        n = 0.0\n",
    "        sum_a = 0.0\n",
    "        sum_b = 0.0\n",
    "        sum_ab = 0.0\n",
    "        sum_a_sq = 0.0\n",
    "        sum_b_sq = 0.0\n",
    "        for idx in range(bitset_size):\n",
    "            if bitset[idx] != 0:\n",
    "                for pos in range(64):\n",
    "                    if (bitset[idx] & (np.int64(1) << np.int64(pos))) != 0:\n",
    "                        user_idx = idx * 64 + pos\n",
    "                        score_for_qs = 0.0\n",
    "                        for q in qs_combinations[i]:\n",
    "                            score_for_qs += score_matrix[user_idx, q]\n",
    "                        score_for_user = grand_totals[user_idx]\n",
    "                        n += 1.0\n",
    "                        sum_a += score_for_qs\n",
    "                        sum_b += score_for_user\n",
    "                        sum_ab += score_for_qs * score_for_user\n",
    "                        sum_a_sq += score_for_qs * score_for_qs\n",
    "                        sum_b_sq += score_for_user * score_for_user\n",
    "        num = n * sum_ab - sum_a * sum_b\n",
    "        den = np.sqrt(n * sum_a_sq - sum_a**2) * np.sqrt(n * sum_b_sq - sum_b**2)\n",
    "        corrs[i] = np.nan if den == 0 else num / den\n",
    "    return corrs\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset,\n",
    "    compute_corrs,\n",
    "    \"numba, bitsets, acculumation instead of arrays, inline, with parallel=False\",\n",
    "    run_line_profiler=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit(boundscheck=False, fastmath=True, nogil=True)\n",
    "def bitset_create(size):\n",
    "    size_in_int64 = int(np.ceil(size / 64))\n",
    "    return np.zeros(size_in_int64, dtype=np.int64)\n",
    "\n",
    "\n",
    "@numba.njit(boundscheck=False, fastmath=True, nogil=True)\n",
    "def bitset_add(arr, pos):\n",
    "    int64_idx = pos // 64\n",
    "    pos_in_int64 = pos % 64\n",
    "    arr[int64_idx] |= np.int64(1) << np.int64(pos_in_int64)\n",
    "\n",
    "\n",
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data[\"user\"] = data[\"user\"].map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data[\"question\"] = data[\"question\"].map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "\n",
    "    all_qs = data.question.unique()\n",
    "    grand_totals = data.groupby(\"user\").score.sum().values\n",
    "\n",
    "    # create bitsets\n",
    "    users_who_answered_q = np.array(\n",
    "        [bitset_create(data.user.nunique()) for _ in range(data.question.nunique())]\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].values:\n",
    "        bitset_add(users_who_answered_q[q], u)\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (data.user.nunique(), data.question.nunique()), dtype=np.int64\n",
    "    )\n",
    "    for row in data.itertuples():\n",
    "        score_matrix[row.user, row.question] = row.score\n",
    "\n",
    "    # todo, would be nice to have a super fast iterator / generator in numba\n",
    "    # rather than creating the whole array\n",
    "    qs_combinations = []\n",
    "    for i, qs in enumerate(itertools.combinations(all_qs, K)):\n",
    "        if i == max_iter:\n",
    "            break\n",
    "        qs_combinations.append(qs)\n",
    "    qs_combinations = np.array(qs_combinations)\n",
    "\n",
    "    start = time.time()\n",
    "    r_vals = compute_corrs(\n",
    "        qs_combinations, users_who_answered_q, score_matrix, grand_totals\n",
    "    )\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame({\"qs\": [tuple(qs) for qs in qs_combinations], \"r\": r_vals})\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "@numba.njit(boundscheck=False, fastmath=True, parallel=True, nogil=True)\n",
    "def compute_corrs(qs_combinations, users_who_answered_q, score_matrix, grand_totals):\n",
    "    num_qs = qs_combinations.shape[0]\n",
    "    bitset_size = users_who_answered_q[0].shape[0]\n",
    "    corrs = np.empty(qs_combinations.shape[0], dtype=np.float64)\n",
    "    for i in numba.prange(num_qs):\n",
    "        # bitset will contain users who answered all questions in qs_array[i]\n",
    "        bitset = users_who_answered_q[qs_combinations[i, 0]].copy()\n",
    "        for q in qs_combinations[i, 1:]:\n",
    "            bitset &= users_who_answered_q[q]\n",
    "        # retrieve stats for the users and compute corrcoef\n",
    "        n = 0.0\n",
    "        sum_a = 0.0\n",
    "        sum_b = 0.0\n",
    "        sum_ab = 0.0\n",
    "        sum_a_sq = 0.0\n",
    "        sum_b_sq = 0.0\n",
    "        for idx in range(bitset_size):\n",
    "            if bitset[idx] != 0:\n",
    "                for pos in range(64):\n",
    "                    if (bitset[idx] & (np.int64(1) << np.int64(pos))) != 0:\n",
    "                        score_for_qs = 0.0\n",
    "                        for q in qs_combinations[i]:\n",
    "                            score_for_qs += score_matrix[idx * 64 + pos, q]\n",
    "                        score_for_user = grand_totals[idx * 64 + pos]\n",
    "                        n += 1.0\n",
    "                        sum_a += score_for_qs\n",
    "                        sum_b += score_for_user\n",
    "                        sum_ab += score_for_qs * score_for_user\n",
    "                        sum_a_sq += score_for_qs * score_for_qs\n",
    "                        sum_b_sq += score_for_user * score_for_user\n",
    "        num = n * sum_ab - sum_a * sum_b\n",
    "        den = np.sqrt(n * sum_a_sq - sum_a**2) * np.sqrt(n * sum_b_sq - sum_b**2)\n",
    "        corrs[i] = np.nan if den == 0 else num / den\n",
    "    return corrs\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset,\n",
    "    compute_corrs,\n",
    "    \"numba, bitsets, acculumation instead of arrays, inline, with parallel=True\",\n",
    "    run_line_profiler=False,\n",
    "    num_iter=5_000_000,  # to match the original blog post\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok.. without precomputing Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit(boundscheck=False, fastmath=True, nogil=True)\n",
    "def bitset_create(size):\n",
    "    size_in_int64 = int(np.ceil(size / 64))\n",
    "    return np.zeros(size_in_int64, dtype=np.int64)\n",
    "\n",
    "\n",
    "@numba.njit(boundscheck=False, fastmath=True, nogil=True)\n",
    "def bitset_add(arr, pos):\n",
    "    int64_idx = pos // 64\n",
    "    pos_in_int64 = pos % 64\n",
    "    arr[int64_idx] |= np.int64(1) << np.int64(pos_in_int64)\n",
    "\n",
    "\n",
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data[\"user\"] = data[\"user\"].map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data[\"question\"] = data[\"question\"].map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "\n",
    "    all_qs = data.question.unique()\n",
    "    grand_totals = data.groupby(\"user\").score.sum().values\n",
    "\n",
    "    # create bitsets\n",
    "    users_who_answered_q = np.array(\n",
    "        [bitset_create(data.user.nunique()) for _ in range(data.question.nunique())]\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].values:\n",
    "        bitset_add(users_who_answered_q[q], u)\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (data.user.nunique(), data.question.nunique()), dtype=np.bool_\n",
    "    )\n",
    "    for row in data.itertuples():\n",
    "        score_matrix[row.user, row.question] = row.score\n",
    "\n",
    "    # todo, would be nice to have a super fast iterator / generator in numba\n",
    "    # rather than creating the whole array\n",
    "    num_qs = all_qs.shape[0]\n",
    "    start = time.time()\n",
    "    r_vals, qs_combinations = compute_corrs(\n",
    "        num_qs, max_iter, users_who_answered_q, score_matrix, grand_totals\n",
    "    )\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame({\"qs\": [tuple(x) for x in qs_combinations], \"r\": r_vals})\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "@numba.njit(boundscheck=False, fastmath=True, nogil=True)\n",
    "def compute_combinations(num_qs, max_iter):\n",
    "    qs_combinations = np.empty((max_iter, 5), dtype=np.int64)\n",
    "    idx = 0\n",
    "    for i in range(num_qs):\n",
    "        for j in range(i + 1, num_qs):\n",
    "            for k in range(j + 1, num_qs):\n",
    "                for l in range(k + 1, num_qs):\n",
    "                    for m in range(l + 1, num_qs):\n",
    "                        qs_combinations[idx, 0] = i\n",
    "                        qs_combinations[idx, 1] = j\n",
    "                        qs_combinations[idx, 2] = k\n",
    "                        qs_combinations[idx, 3] = l\n",
    "                        qs_combinations[idx, 4] = m\n",
    "                        idx += 1\n",
    "                        if idx >= max_iter:\n",
    "                            return qs_combinations\n",
    "\n",
    "\n",
    "@numba.njit(boundscheck=False, fastmath=True, parallel=True, nogil=True)\n",
    "def compute_corrs(num_qs, max_iter, users_who_answered_q, score_matrix, grand_totals):\n",
    "    bitset_size = users_who_answered_q[0].shape[0]\n",
    "    corrs = np.empty(max_iter, dtype=np.float64)\n",
    "    qs_combinations = compute_combinations(num_qs, max_iter)\n",
    "    for i in numba.prange(max_iter):\n",
    "        # bitset will contain users who answered all questions in qs_array[i]\n",
    "        bitset = users_who_answered_q[qs_combinations[i, 0]].copy()\n",
    "        for q in qs_combinations[i, 1:]:\n",
    "            bitset &= users_who_answered_q[q]\n",
    "        # retrieve stats for the users and compute corrcoef\n",
    "        n = 0.0\n",
    "        sum_a = 0.0\n",
    "        sum_b = 0.0\n",
    "        sum_ab = 0.0\n",
    "        sum_a_sq = 0.0\n",
    "        sum_b_sq = 0.0\n",
    "        for idx in range(bitset_size):\n",
    "            if bitset[idx] != 0:\n",
    "                for pos in range(64):\n",
    "                    if (bitset[idx] & (np.int64(1) << np.int64(pos))) != 0:\n",
    "                        score_for_qs = 0.0\n",
    "                        for q in qs_combinations[i]:\n",
    "                            score_for_qs += score_matrix[idx * 64 + pos, q]\n",
    "                        score_for_user = grand_totals[idx * 64 + pos]\n",
    "                        n += 1.0\n",
    "                        sum_a += score_for_qs\n",
    "                        sum_b += score_for_user\n",
    "                        sum_ab += score_for_qs * score_for_user\n",
    "                        sum_a_sq += score_for_qs * score_for_qs\n",
    "                        sum_b_sq += score_for_user * score_for_user\n",
    "        num = n * sum_ab - sum_a * sum_b\n",
    "        den = np.sqrt(n * sum_a_sq - sum_a**2) * np.sqrt(n * sum_b_sq - sum_b**2)\n",
    "        corrs[i] = np.nan if den == 0 else num / den\n",
    "    return corrs, qs_combinations\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset,\n",
    "    compute_corrs,\n",
    "    \"numba, without precomputing qs\",\n",
    "    run_line_profiler=False,\n",
    "    num_iter=5_000_000,  # to match the original blog post\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data[\"user\"] = data[\"user\"].map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data[\"question\"] = data[\"question\"].map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "\n",
    "    all_qs = data.question.unique()\n",
    "    grand_totals = data.groupby(\"user\").score.sum().values\n",
    "\n",
    "    # create bitsets\n",
    "    users_who_answered_q = np.array(\n",
    "        [bitset_create(data.user.nunique()) for _ in range(data.question.nunique())]\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].values:\n",
    "        bitset_add(users_who_answered_q[q], u)\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (data.user.nunique(), data.question.nunique()), dtype=np.bool_\n",
    "    )\n",
    "    for row in data.itertuples():\n",
    "        score_matrix[row.user, row.question] = row.score\n",
    "\n",
    "    # todo, would be nice to have a super fast iterator / generator in numba\n",
    "    # rather than creating the whole array\n",
    "    num_qs = all_qs.shape[0]\n",
    "    start = time.time()\n",
    "    r_vals, qs_combinations = compute_corrs(\n",
    "        num_qs, max_iter, users_who_answered_q, score_matrix, grand_totals\n",
    "    )\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame({\"qs\": [tuple(x) for x in qs_combinations], \"r\": r_vals})\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "@numba.njit(boundscheck=False, fastmath=True, parallel=True, nogil=True)\n",
    "def compute_corrs(num_qs, max_iter, users_who_answered_q, score_matrix, grand_totals):\n",
    "    bitset_size = users_who_answered_q[0].shape[0]\n",
    "    corrs = np.empty(max_iter, dtype=np.float64)\n",
    "\n",
    "    # Compute combinations inline\n",
    "    qs_combinations = np.empty((max_iter, 5), dtype=np.int64)\n",
    "    try:\n",
    "        idx = 0\n",
    "        for i in range(num_qs):\n",
    "            for j in range(i + 1, num_qs):\n",
    "                for k in range(j + 1, num_qs):\n",
    "                    for l in range(k + 1, num_qs):\n",
    "                        for m in range(l + 1, num_qs):\n",
    "                            qs_combinations[idx, 0] = i\n",
    "                            qs_combinations[idx, 1] = j\n",
    "                            qs_combinations[idx, 2] = k\n",
    "                            qs_combinations[idx, 3] = l\n",
    "                            qs_combinations[idx, 4] = m\n",
    "                            idx += 1\n",
    "                            if idx >= max_iter:\n",
    "                                raise Exception\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    for i in numba.prange(max_iter):\n",
    "        # bitset will contain users who answered all questions in qs_array[i]\n",
    "        bitset = users_who_answered_q[qs_combinations[i, 0]].copy()\n",
    "        for q in qs_combinations[i, 1:]:\n",
    "            bitset &= users_who_answered_q[q]\n",
    "        # retrieve stats for the users and compute corrcoef\n",
    "        n = 0.0\n",
    "        sum_a = 0.0\n",
    "        sum_b = 0.0\n",
    "        sum_ab = 0.0\n",
    "        sum_a_sq = 0.0\n",
    "        sum_b_sq = 0.0\n",
    "        for idx in range(bitset_size):\n",
    "            if bitset[idx] != 0:\n",
    "                for pos in range(64):\n",
    "                    if (bitset[idx] & (np.int64(1) << np.int64(pos))) != 0:\n",
    "                        score_for_qs = 0.0\n",
    "                        for q in qs_combinations[i]:\n",
    "                            score_for_qs += score_matrix[idx * 64 + pos, q]\n",
    "                        score_for_user = grand_totals[idx * 64 + pos]\n",
    "                        n += 1.0\n",
    "                        sum_a += score_for_qs\n",
    "                        sum_b += score_for_user\n",
    "                        sum_ab += score_for_qs * score_for_user\n",
    "                        sum_a_sq += score_for_qs * score_for_qs\n",
    "                        sum_b_sq += score_for_user * score_for_user\n",
    "        num = n * sum_ab - sum_a * sum_b\n",
    "        den = np.sqrt(n * sum_a_sq - sum_a**2) * np.sqrt(n * sum_b_sq - sum_b**2)\n",
    "        corrs[i] = np.nan if den == 0 else num / den\n",
    "    return corrs, qs_combinations\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset,\n",
    "    compute_corrs,\n",
    "    \"numba, computing qs inline\",\n",
    "    run_line_profiler=False,\n",
    "    num_iter=5_000_000,  # to match the original blog post\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_corrset(data, K, max_iter=1000):\n",
    "    data = data.copy()\n",
    "    data[\"user\"] = data[\"user\"].map({u: i for i, u in enumerate(data.user.unique())})\n",
    "    data[\"question\"] = data[\"question\"].map(\n",
    "        {q: i for i, q in enumerate(data.question.unique())}\n",
    "    )\n",
    "\n",
    "    all_qs = data.question.unique()\n",
    "    grand_totals = data.groupby(\"user\").score.sum().values\n",
    "\n",
    "    # create bitsets\n",
    "    users_who_answered_q = np.array(\n",
    "        [bitset_create(data.user.nunique()) for _ in range(data.question.nunique())]\n",
    "    )\n",
    "    for q, u in data[[\"question\", \"user\"]].values:\n",
    "        bitset_add(users_who_answered_q[q], u)\n",
    "\n",
    "    score_matrix = np.zeros(\n",
    "        (data.user.nunique(), data.question.nunique()), dtype=np.bool_\n",
    "    )\n",
    "    for row in data.itertuples():\n",
    "        score_matrix[row.user, row.question] = row.score\n",
    "\n",
    "    # todo, would be nice to have a super fast iterator / generator in numba\n",
    "    # rather than creating the whole array\n",
    "    num_qs = all_qs.shape[0]\n",
    "    start = time.time()\n",
    "    r_vals = compute_corrs(\n",
    "        num_qs, max_iter, users_who_answered_q, score_matrix, grand_totals\n",
    "    )\n",
    "    avg_iter_time_secs = (time.time() - start) / max_iter\n",
    "    corrs = pd.DataFrame({\"r\": r_vals})\n",
    "    return corrs, avg_iter_time_secs\n",
    "\n",
    "\n",
    "@numba.njit(boundscheck=False, fastmath=True, parallel=False, nogil=True)\n",
    "def compute_corrs(num_qs, max_iter, users_who_answered_q, score_matrix, grand_totals):\n",
    "    bitset_size = users_who_answered_q[0].shape[0]\n",
    "    corrs = np.empty(max_iter, dtype=np.float64)\n",
    "    # Compute combinations inline\n",
    "    outer_idx = 0\n",
    "    for i in range(num_qs):\n",
    "        for j in range(i + 1, num_qs):\n",
    "            for k in range(j + 1, num_qs):\n",
    "                for l in numba.prange(k + 1, num_qs):\n",
    "                    for m in numba.prange(l + 1, num_qs):\n",
    "                        qs_combination = np.array([i, j, k, l, m])\n",
    "                        # bitset will contain users who answered all questions in qs_array[i]\n",
    "                        bitset = users_who_answered_q[qs_combination[0]].copy()\n",
    "                        for q in qs_combination:\n",
    "                            bitset &= users_who_answered_q[q]\n",
    "                        # retrieve stats for the users and compute corrcoef\n",
    "                        n = 0.0\n",
    "                        sum_a = 0.0\n",
    "                        sum_b = 0.0\n",
    "                        sum_ab = 0.0\n",
    "                        sum_a_sq = 0.0\n",
    "                        sum_b_sq = 0.0\n",
    "                        for idx in range(bitset_size):\n",
    "                            if bitset[idx] != 0:\n",
    "                                for pos in range(64):\n",
    "                                    if (\n",
    "                                        bitset[idx] & (np.int64(1) << np.int64(pos))\n",
    "                                    ) != 0:\n",
    "                                        score_for_qs = 0.0\n",
    "                                        for q in qs_combination:\n",
    "                                            score_for_qs += score_matrix[\n",
    "                                                idx * 64 + pos, q\n",
    "                                            ]\n",
    "                                        score_for_user = grand_totals[idx * 64 + pos]\n",
    "                                        n += 1.0\n",
    "                                        sum_a += score_for_qs\n",
    "                                        sum_b += score_for_user\n",
    "                                        sum_ab += score_for_qs * score_for_user\n",
    "                                        sum_a_sq += score_for_qs * score_for_qs\n",
    "                                        sum_b_sq += score_for_user * score_for_user\n",
    "                        num = n * sum_ab - sum_a * sum_b\n",
    "                        den = np.sqrt(n * sum_a_sq - sum_a**2) * np.sqrt(\n",
    "                            n * sum_b_sq - sum_b**2\n",
    "                        )\n",
    "                        corrs[outer_idx] = np.nan if den == 0 else num / den\n",
    "                        outer_idx += 1\n",
    "                        if outer_idx >= max_iter:\n",
    "                            return corrs\n",
    "\n",
    "\n",
    "run_benchmark(\n",
    "    k_corrset,\n",
    "    compute_corrs,\n",
    "    \"numba, combinations inline\",\n",
    "    run_line_profiler=False,\n",
    "    # num_iter=10,  # to match the original blog post\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
